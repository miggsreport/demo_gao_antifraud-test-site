{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAO Fraud Ontology - SHACL Validation\n",
    "\n",
    "This notebook validates the GAO fraud ontology instance data against SHACL shapes.\n",
    "\n",
    "**Phase 1: Foundation Classes**\n",
    "- FraudActivity\n",
    "- FederalAgency\n",
    "- FederalUnit\n",
    "- ProgramArea\n",
    "- FundingStream\n",
    "- RevenueStream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyshacl if not already installed\n",
    "!pip install pyshacl rdflib pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/claude')\n",
    "\n",
    "from validate_ontology import SHACLValidator\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these paths to match your file locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths - UPDATE THESE\n",
    "DATA_FILE = \"/home/claude/gfo_turtle.ttl\"  # Your ontology file\n",
    "SHAPES_FILE = \"/home/claude/phase1_foundation_shapes.ttl\"  # Phase 1 shapes\n",
    "OUTPUT_DIR = \"/home/claude/validation_reports\"\n",
    "\n",
    "# Validation settings\n",
    "INFERENCE = 'none'  # Options: 'none', 'rdfs', 'owlrl'\n",
    "\n",
    "# Check files exist\n",
    "assert Path(DATA_FILE).exists(), f\"Data file not found: {DATA_FILE}\"\n",
    "assert Path(SHAPES_FILE).exists(), f\"Shapes file not found: {SHAPES_FILE}\"\n",
    "\n",
    "print(\"âœ“ Configuration complete\")\n",
    "print(f\"  Data: {DATA_FILE}\")\n",
    "print(f\"  Shapes: {SHAPES_FILE}\")\n",
    "print(f\"  Inference: {INFERENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Validation\n",
    "\n",
    "This will validate your ontology and produce detailed reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validator\n",
    "validator = SHACLValidator(\n",
    "    data_file=DATA_FILE,\n",
    "    shapes_file=SHAPES_FILE,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "# Run full validation\n",
    "conforms, results, summary = validator.run_full_validation(\n",
    "    inference=INFERENCE,\n",
    "    save_reports=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "if results:\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(f\"Total issues: {len(df_results)}\")\n",
    "    df_results.head(20)\n",
    "else:\n",
    "    print(\"No validation issues found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Show only Violations\n",
    "    violations = df_results[df_results['severity'] == 'Violation']\n",
    "    print(f\"\\nViolations: {len(violations)}\")\n",
    "    violations.head(20)\n",
    "else:\n",
    "    print(\"No violations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Show only Warnings\n",
    "    warnings = df_results[df_results['severity'] == 'Warning']\n",
    "    print(f\"\\nWarnings: {len(warnings)}\")\n",
    "    warnings.head(20)\n",
    "else:\n",
    "    print(\"No warnings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze by Class Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Extract class name from focus node\n",
    "    df_results['class_name'] = df_results['focus_node'].str.split('/').str[-1]\n",
    "    \n",
    "    # Count issues by class\n",
    "    class_issues = df_results.groupby(['class_name', 'severity']).size().unstack(fill_value=0)\n",
    "    class_issues['Total'] = class_issues.sum(axis=1)\n",
    "    class_issues = class_issues.sort_values('Total', ascending=False)\n",
    "    \n",
    "    print(\"\\nIssues by Class Type:\")\n",
    "    print(class_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze by Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Count issues by property\n",
    "    property_issues = df_results.groupby(['result_path', 'severity']).size().unstack(fill_value=0)\n",
    "    property_issues['Total'] = property_issues.sum(axis=1)\n",
    "    property_issues = property_issues.sort_values('Total', ascending=False)\n",
    "    \n",
    "    print(\"\\nIssues by Property:\")\n",
    "    print(property_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Specific Issues for Fixing\n",
    "\n",
    "Export violations for a specific property to a CSV file for easy fixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Example: Export all missing label violations\n",
    "    missing_labels = df_results[\n",
    "        (df_results['result_path'] == 'label') & \n",
    "        (df_results['severity'] == 'Violation')\n",
    "    ]\n",
    "    \n",
    "    if not missing_labels.empty:\n",
    "        output_file = Path(OUTPUT_DIR) / \"missing_labels.csv\"\n",
    "        missing_labels[['focus_node', 'focus_node_label', 'message']].to_csv(\n",
    "            output_file, \n",
    "            index=False\n",
    "        )\n",
    "        print(f\"âœ“ Exported {len(missing_labels)} missing label issues to: {output_file}\")\n",
    "    else:\n",
    "        print(\"No missing label issues found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Stats for Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all classes being validated\n",
    "from rdflib import Namespace\n",
    "\n",
    "SH = Namespace(\"http://www.w3.org/ns/shacl#\")\n",
    "RDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "\n",
    "# Query for target classes in shapes\n",
    "target_classes_query = \"\"\"\n",
    "    PREFIX sh: <http://www.w3.org/ns/shacl#>\n",
    "    SELECT DISTINCT ?targetClass\n",
    "    WHERE {\n",
    "        ?shape sh:targetClass ?targetClass .\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "stats = []\n",
    "for row in validator.shapes_graph.query(target_classes_query):\n",
    "    target_class = row.targetClass\n",
    "    class_name = str(target_class).split('/')[-1]\n",
    "    \n",
    "    # Count instances\n",
    "    count_query = f\"\"\"\n",
    "        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        SELECT (COUNT(DISTINCT ?instance) as ?count)\n",
    "        WHERE {{\n",
    "            ?instance rdf:type <{target_class}> .\n",
    "        }}\n",
    "    \"\"\"\n",
    "    \n",
    "    count_result = list(validator.data_graph.query(count_query))\n",
    "    instance_count = int(count_result[0][0]) if count_result else 0\n",
    "    \n",
    "    # Count issues for this class\n",
    "    if results:\n",
    "        class_issues_count = len([r for r in results if class_name in r['focus_node']])\n",
    "    else:\n",
    "        class_issues_count = 0\n",
    "    \n",
    "    stats.append({\n",
    "        'Class': class_name,\n",
    "        'Instances': instance_count,\n",
    "        'Issues': class_issues_count,\n",
    "        'Clean': 'âœ“' if class_issues_count == 0 else 'âœ—'\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats)\n",
    "print(\"\\nValidation Statistics by Class:\")\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Run this cell for a final summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Overall Conforms: {'âœ“ YES' if conforms else 'âœ— NO'}\")\n",
    "print(f\"Total Issues: {summary['total_issues']}\")\n",
    "if summary['total_issues'] > 0:\n",
    "    print(\"\\nBy Severity:\")\n",
    "    for severity, count in summary['by_severity'].items():\n",
    "        print(f\"  {severity}: {count}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if summary['total_issues'] == 0:\n",
    "    print(\"\\nðŸŽ‰ Congratulations! All Phase 1 validation checks passed!\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Detailed reports saved to: {OUTPUT_DIR}/\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Review violations first (highest priority)\")\n",
    "    print(\"  2. Fix data issues in your ontology\")\n",
    "    print(\"  3. Re-run validation to verify fixes\")\n",
    "    print(\"  4. Move on to warnings and info items\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
